# llm-cortex

![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)
![Language: Go](https://img.shields.io/badge/Language-Go-blue.svg)
![Language: Shell](https://img.shields.io/badge/Language-Shell-lightgrey.svg)
![Language: Python](https://img.shields.io/badge/Language-Python-3776AB.svg)
[![Contributions Welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)](https://github.com/owen-6936/llm-cortex/issues)
![Last Commit](https://img.shields.io/github/last-commit/owen-6936/llm-cortex)
![Repo Size](https://img.shields.io/github/repo-size/owen-6936/llm-cortex)

## Introduction

LLM-Cortex is a high-performance Go framework designed to manage and serve machine learning models. It provides a robust backend for orchestrating persistent Python processes, enabling efficient, parallel execution of models like BLIP, CLIP, and CLIPtion.

## Overview

A modular shell for orchestrating local models. It uses Go for GGUF-based LLMs and Python for other modalities like vision and audio, with swap-aware memory management, plugin-centric routing, and model lifecycle control.
The Go application acts as the central orchestrator, calling standalone Python scripts for specific tasks like image captioning.

## Features

- ğŸŒ **Multi-language Orchestration:** Uses Go for high-performance LLM management and calls standalone Python scripts for vision/audio tasks.
- ğŸ§  Model registry with plugin intent mapping
- ğŸ’¾ Swap provisioning and memory-aware loading
- ğŸ” Sequential and fallback model orchestration
- ğŸ› ï¸ Scripts for swap monitoring and CLI wrapping
- ğŸš€ **Parallel Execution**: A built-in scheduler (`TaskRunner`) allows for concurrent execution of multiple models, dramatically improving throughput.
- âš™ï¸ **Persistent Model Serving**: Manages ML models as long-running interactive Python processes, eliminating model-loading overhead for sequential requests.
- ğŸ› ï¸ **Generic Process Spawning**: The `spawn` package provides a low-level, reusable component for managing any interactive command-line process from Go.
- ğŸ› ï¸ **Centralized Configuration & Error Handling**: Easily configure paths and benefit from robust, session-based logging for `stdout` and `stderr`.

## Getting Started

1. **Clone the repository:**

    ```bash
    git clone https://github.com/owen-6936/llm-cortex.git
    cd llm-cortex
    ```

```bash
chmod +x ./setup.sh
./setup.sh
```

## Directory Structure

```folder structure
llm-cortex/
â”œâ”€â”€ core/
â”‚   â””â”€â”€ models/
â”‚       â””â”€â”€ vision/       # Go wrappers for vision models
â”œâ”€â”€ examples/             # Example usage scripts
â”œâ”€â”€ handlers/             # HTTP handlers for the web server
â”œâ”€â”€ models/               # Directory for storing model files
â”‚   â””â”€â”€ clip-vit-b-32/                  # Example model directory
â”‚       â””â”€â”€ config.json                 # Example model config file
â”‚       â””â”€â”€ model.safetensors           # Example GGUF model file
â”‚       â””â”€â”€ open_clip_config.json       # Example OpenCLIP config file
â”‚       â””â”€â”€ preprocessor_config.json    # Example preprocess config file
â”‚       â””â”€â”€ special_tokens_map.json     # Example special tokens file
â”‚       â””â”€â”€ tokenizer_config.json       # Example tokenizer config file
â”‚       â””â”€â”€ tokenizer.json              # Example tokenizer file
â”‚       â””â”€â”€ vocab.json                  # Example vocab file
â”‚       â””â”€â”€ qwen/                       # Another example model directory
â”‚           â””â”€â”€ Qwen2.5-Coder-7B-Instruct-Q6_K_L.gguf                 # Example model config file
â”‚           â””â”€â”€ Qwen2.5-VL-7B-Instruct-Q6_K.gguf                      # Example GGUF model file
â”‚   â””â”€â”€ ...                             # ...Other model directories
â”œâ”€â”€ python/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ vision/       # Python scripts for vision models (e.g., blip.py)
â”‚   â””â”€â”€ requirements.txt  # Python dependencies
â”œâ”€â”€ router/               # Old Go orchestration logic
â”œâ”€â”€ scripts/              # Bash helpers
â”‚   â”œâ”€â”€ setup_swap.sh    # Script to set up swap file
â”‚   â”œâ”€â”€ teardown_swap.sh # Script to remove swap file
â”œâ”€â”€ samples/              # Sample data for testing
â”œâ”€â”€ scheduler/            # Parallel task execution scheduler
â”œâ”€â”€ spawn/                # Low-level process management
â”œâ”€â”€ scripts/              # Bash helpers
â”œâ”€â”€ ui/                   # UI assets for the web server
â”œâ”€â”€ utils/                # Utility functions
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ setup.sh              # Project setup and dependency installer
â”œâ”€â”€ python-setup.sh       # Python environment setup script
â””â”€â”€ main.go               # Main application entry point
```

## models/

This folder is for local models used by the orchestration shell.

**Do not commit model files.** They are large and system-specific.

To use:

1. Download GGUF models from Hugging Face
2. Place them here following the structure in `router/model_registry.go`

## Contributing

Contributions are welcome! Please open issues or pull requests for enhancements or bug fixes.

## License

MIT
See [LICENSE](LICENSE) for details.
