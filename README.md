# llm-cortex

A modular shell for orchestrating local GGUF-based LLMs with swap-aware memory management, plugin-centric routing, and model lifecycle control.

## Features

- ğŸ§  Model registry with plugin intent mapping
- ğŸ’¾ Swap provisioning and memory-aware loading
- ğŸ” Sequential and fallback model orchestration
- ğŸ› ï¸ Scripts for swap monitoring and CLI wrapping

## Getting Started

1. Provision swap (see `scripts/swap-monitor.sh`)
2. Build `llama.cpp` and copy `llama-cli` here
3. Place GGUF models in `models/`
4. Run `go run router/cortex_shell.go`

## Directory Structure

```llm-cortex

llm-cortex/
â”œâ”€â”€ bin/                  # All binaries live here
â”‚   â”œâ”€â”€ llama-cli
â”‚   â”œâ”€â”€ llama-server
â”‚   â””â”€â”€ llama-bench
â”œâ”€â”€ models/               # GGUF models
â”œâ”€â”€ router/               # Go orchestration logic
â”‚   â”œâ”€â”€ cortex_shell.go
â”‚   â”œâ”€â”€ model_registry.go
â”‚   â””â”€â”€ memory_manager.go
â”œâ”€â”€ scripts/              # Bash helpers
â”‚   â””â”€â”€ swap-monitor.sh
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE

```

## models/

This folder is for local GGUF models used by the orchestration shell.

**Do not commit model files.** They are large and system-specific.

To use:

1. Download GGUF models from Hugging Face
2. Place them here following the structure in `router/model_registry.go`

## Contributing

Contributions are welcome! Please open issues or pull requests for enhancements or bug fixes.

## License

MIT
See [LICENSE](LICENSE) for details.
